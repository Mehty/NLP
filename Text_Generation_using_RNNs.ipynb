{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation using RNNs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehty/NLP/blob/master/Text_Generation_using_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dwvKM9q-_iRW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import re\n",
        "import pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1EQfguXJCk3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "313b3710-8f8d-4d72-d782-4fa33dfc8db4"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k9KqAoiPFTwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "ea6ae850-5e0c-440a-aafd-bd4170840980"
      },
      "cell_type": "code",
      "source": [
        "with open(data_path, 'r', encoding = \"ISO-8859-1\") as f:\n",
        "  print(f.read())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pT8GGoE-BKD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "42ce3d5f-90fe-4a1b-d241-81b3b9565be5"
      },
      "cell_type": "code",
      "source": [
        "# Load date\n",
        "data_path = '/content/drive/My Drive/Colab Notebooks/data/ner_dataset.csv'\n",
        "data = pandas.read_csv(data_path, encoding = \"ISO-8859-1\")\n",
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence #           Word  POS Tag\n",
              "0  Sentence: 1      Thousands  NNS   O\n",
              "1          NaN             of   IN   O\n",
              "2          NaN  demonstrators  NNS   O\n",
              "3          NaN           have  VBP   O\n",
              "4          NaN        marched  VBN   O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "4gHSZBlIGil7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "697f5685-6b4e-4456-8b45-6caa1201383c"
      },
      "cell_type": "code",
      "source": [
        "words = data.Word\n",
        "tags = data.Tag\n",
        "\n",
        "names = []\n",
        "for word, tag in zip(words, tags):\n",
        "  if tag == 'O': continue\n",
        "  if not tag.endswith('-per'): continue\n",
        "  if tag == 'B-per':\n",
        "    names.append(word)\n",
        "  else:\n",
        "    names[-1] = names[-1] + ' ' + word\n",
        "\n",
        "print(len(names))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "danYqV8QHr3T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a2e583e6-c4b4-4c10-e9b7-bbcb64426e07"
      },
      "cell_type": "code",
      "source": [
        "# Only alphabets, hyphens and spaces allowed.\n",
        "# Minimum 2 words. (Filters out names like \"Bush\")\n",
        "names = [name for name in names if re.match('^[A-Za-z\\- ]*$', name)]\n",
        "names = list(set(names)) # unique\n",
        "print('Number of names:', len(names))\n",
        " \n",
        "train_data = names[:3600]\n",
        "test_data = names[3600:4000]\n",
        " \n",
        "print(len(train_data), train_data[:10])\n",
        " \n",
        "all_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ- $' # $ denotes BOS / EOS\n",
        "n_letters = len(all_letters)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of names: 5631\n",
            "3600 ['NE WIN', 'Sao Tome', 'Ireju Bares', 'Dennis Weaver', 'Chanderpaul', 'Githongo', 'Kevin James', 'Jean-Baptiste Natama', 'Larry Paul Fidler', 'Franz Klammer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mieQomFXIUwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1529
        },
        "outputId": "0804a49e-c04b-4253-def7-38a9e657fa04"
      },
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "## Creating the Network\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        " \n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        " \n",
        "        ## declare the layers\n",
        "        # rnn layer 1. input = input + hidden values from previous time step\n",
        "        self.rnn1 = nn.Linear(input_size+hidden_size, hidden_size) \n",
        "        # rnn layer 2. input = hidden values from rnn1 + hidden values from previous time step\n",
        "        self.rnn2 = nn.Linear(hidden_size + hidden_size, hidden_size) \n",
        "        # final layer. output from rnn2 to final output  \n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        " \n",
        "    def forward(self, input, hidden):\n",
        "        hidden1 = F.sigmoid(self.rnn1(torch.cat((input, hidden[0]), 1)))\n",
        "        # dropout layer (optional) \n",
        "        hidden1 = F.dropout(hidden1, p=0.2, training=self.training)\n",
        "        \n",
        "        hidden2 = F.sigmoid(self.rnn2(torch.cat((hidden1, hidden[1]), 1)))\n",
        "        # dropout layer (optional)\n",
        "        hidden2 = F.dropout(hidden2, p=0.2, training=self.training)\n",
        "        output = self.softmax(self.output(hidden2))\n",
        "        return output, (hidden1, hidden2)\n",
        " \n",
        "    def init_hidden(self):\n",
        "        # initialize hidden layers to 0 at the beginning\n",
        "        return (Variable(torch.zeros(1, self.hidden_size)), Variable(torch.zeros(1, self.hidden_size)))\n",
        " \n",
        "rnn = RNN(n_letters, 128, n_letters)\n",
        "criterion = nn.NLLLoss() # negative log likelihood\n",
        " \n",
        "###############################################################################\n",
        "## Preparing Data for Training\n",
        " \n",
        "# One-hot matrix of first to last letters (+BOS) for input\n",
        "def input_tensor(name, bos=True):\n",
        "    input = ('$' + name) if bos else name\n",
        "    tensor = torch.zeros(len(input), 1, n_letters)\n",
        "    for idx, letter in enumerate(input):\n",
        "        tensor[idx][0][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        " \n",
        "# Index of first letter to last letter (+EOS) for target\n",
        "def target_tensor(name):\n",
        "    target = name + '$'\n",
        "    letter_indexes = [all_letters.find(letter) for letter in target]\n",
        "    return torch.LongTensor(letter_indexes)\n",
        " \n",
        "print(input_tensor('Keshav Dhandhania'))\n",
        "print(target_tensor('Keshav Dhandhania')) \n",
        " "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 1., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0.]]])\n",
            "tensor([36,  4, 18,  7,  0, 21, 53, 29,  7,  0, 13,  3,  7,  0, 13,  8,  0, 54])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tRm8ZKgxK-Vr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Training\n",
        " \n",
        "import sys\n",
        " \n",
        "log_interval = 100\n",
        "learning_rate = 0.01\n",
        "previous_loss = 100.0\n",
        " \n",
        "def train():\n",
        "    global learning_rate, previous_loss\n",
        "    print('Using learning rate: %.5f' % learning_rate)\n",
        " \n",
        "    rnn.train()\n",
        "    running_loss = 0.0\n",
        " \n",
        "    for ii, name in enumerate(train_data):\n",
        "        if ii % log_interval == 0:\n",
        "            print('.', end='')\n",
        "            sys.stdout.flush()\n",
        "        input, target = Variable(input_tensor(name)), Variable(target_tensor(name))\n",
        " \n",
        "        rnn.zero_grad()\n",
        "        hidden = rnn.init_hidden()\n",
        " \n",
        "        loss = 0\n",
        "        for idx in range(len(name)+1):\n",
        "            output, hidden = rnn(input[idx], hidden)\n",
        "            loss += criterion(output, target[idx].unsqueeze(0))\n",
        "        loss.backward()\n",
        " \n",
        "        # torch.nn.utils.clip_grad_norm(rnn.parameters(), 0.25)\n",
        "        for p in rnn.parameters():\n",
        "            p.data.add_(-learning_rate, p.grad.data)\n",
        " \n",
        "        running_loss += loss.item()\n",
        "    print('')\n",
        " \n",
        "    avg_loss = running_loss / sum(len(name) for name in train_data)\n",
        "    if previous_loss < avg_loss: learning_rate *= 0.8\n",
        "    previous_loss = avg_loss\n",
        "    print('Training loss: %.3f' % avg_loss)\n",
        " \n",
        "###############################################################################\n",
        "## Testing\n",
        " \n",
        "def test():\n",
        "    rnn.eval()\n",
        " \n",
        "    running_loss = 0.0\n",
        " \n",
        "    for name in test_data:\n",
        "        input, target = Variable(input_tensor(name)), Variable(target_tensor(name))\n",
        "        hidden = rnn.init_hidden()\n",
        " \n",
        "        loss = 0\n",
        "        for idx in range(len(name)+1):\n",
        "            output, hidden = rnn(input[idx], hidden)\n",
        "            loss += criterion(output, target[idx].unsqueeze(0))\n",
        " \n",
        "        running_loss += loss.item()\n",
        " \n",
        "    avg_loss = running_loss / sum(len(name) for name in test_data)\n",
        "    print('Testing loss: %.3f' % avg_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oJ2Zr5jBLU1N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Sampling the network\n",
        " \n",
        "from numpy.random import choice\n",
        " \n",
        "max_length = 50\n",
        " \n",
        "# Sample given a starting letter\n",
        "def sample(prefix=''):\n",
        "    rnn.eval()\n",
        " \n",
        "    output_string = prefix\n",
        "    ... # get initial hidden vector\n",
        " \n",
        "    for idx in range(max_length):\n",
        "        input = Variable(input_tensor(output_string))\n",
        "        ... # execute one time step of rnn \n",
        " \n",
        "        if idx < len(prefix): continue # still 'feeding' in the prefix, no need to change output_string\n",
        " \n",
        "        probabilities = ... # calculate probabilities from output\n",
        "        sample_idx = ... # sample idx between (0, n_letters) using numpy choice\n",
        "        if sample_idx == n_letters - 1: break # EOS \n",
        " \n",
        "        letter = ... \n",
        "        output_string = ... \n",
        " \n",
        "    print('Prefix = \"%s\".' % prefix, 'Generated string =', output_string)\n",
        "    return output_string\n",
        " \n",
        "# Get multiple samples from one category and multiple starting letters\n",
        "def samples():\n",
        "    for i in range(5):\n",
        "        sample()\n",
        "    for start_letter in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
        "        sample(start_letter)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}